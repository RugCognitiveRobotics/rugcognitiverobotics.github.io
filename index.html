<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Cognitive Robotics Course</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="icon" href="images/favicon.ico">
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="https://www.rug.nl/ocasys/fwn/vak/show?code=WMAI003-05" target="_blank" class="logo"><strong>Cognitive Robotics Course (WMAI19001)</strong> - Hamidreza Kasaei </a>
									<ul class="icons">
										<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon fa-snapchat-ghost"><span class="label">Snapchat</span></a></li>
										<li><a href="#" class="icon fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon fa-medium"><span class="label">Medium</span></a></li>
									</ul>
								</header>

							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h1>Cognitive Robotics</h1>											
										</header>
										<p style="text-align: justify; color: rgb(0, 0, 0);">Cognitive robots are expected to be more autonomous and efficiently work in human-centric environments. For these robots, 
											open-ended learning for object perception and grasping is a challenging task due to the high demand for accurate and real-time response under changing and unpredictable environmental conditions. 
											In this course, “open-ended” implies that the set of object categories to be learned is not known in advance, and the training instances are extracted from online experiences of a robot, and become gradually 
											available over time, rather than being completely available at the beginning of the learning process. This way the robot adapts its perception and grasping skills over time to different environments. 
											</br></br>
											This course covers a diverse set of topics that focus on addressing the most critical aspects of building a cognitive robotic system. Students will practice essential research skills including put
											hands-on experiences, debating, reviewing and critiquing papers, and presenting ideas effectively.
										</p> 
										<b style="color: rgb(255, 0, 25);">*Remark:</b> The course unit prepares students to do their graduation project if they choose to do it in robotics.
										<!--<ul class="actions">
											<li><a href="#" class="button big">Learn More</a></li>
										</ul>-->
									</div>
									<span class="image fit">
										<img src="images/pic10.jpg"/>
									</span>
								</section>

								<section id="CS">
									<header class="major">
										<h2 id="OR">Coordinator</h2>
									</header>
									<div class="content">
										<article>
											<div class="content">
												<p> </p>
												<p style="text-align: justify;"><span class="image left"><img src="images/hamidreza.png" alt=""></span>	
													<h3> <a href="http://www.ai.rug.nl/hkasaei" target="_blank" style="color: rgb(0,0, 0);">Dr. Hamidreza Kasaei</a></h3>
													<p style="text-align: justify;">My research interest lies at the intersection of machine learning, robotics, and machine
													vision, particularly in the area of open-ended learning, 3D object perception, grasp
													affordance detection, and object manipulation. My goal is to achieve a breakthrough in robotics by
													enabling robots to learn from past experiences incrementally and safely interact with human users.
													I have been investigating on active perception, where robots use their mobility and manipulation capabilities not only
													to gain useful perceptual information but also to predict the next-best-view for improving the performance of
													object detection and manipulation. I have evaluated my works on different
													robotic platforms, including PR2, robotic arms (UR5e, Franka, Kinova), and humanoid
													robots. I am leading the <a href="https://www.ai.rug.nl/irl-lab/" target="_blank" style="color: rgb(0,0, 150);">Interactive Robot Learning Laboratory (IRL-Lab)</a> focusing on Lifelong Interactive Robot Learning in the areas of 3D Object Perception, Grasp Affordance, and Object Manipulation.																								
													Navigate <a href="http://www.ai.rug.nl/hkasaei" target="_blank" style="color: rgb(0,0, 150);"> my webpage </a> and 								
													<a href="https://www.ai.rug.nl/irl-lab/" target="_blank" style="color: rgb(0,0, 150);">IRL-Lab page</a>
													if you want to know more about my research.
													
													<p style="text-align: justify; color: rgb(0, 0, 0);">
													I developed the "Cognitive Robotics" course from scratch in 2019 and improving its content constantly.	
													This course covers a diverse set of topics that focus on addressing the most critical aspects of building a cognitive robotic system. 
													We recently wrote a survey paper about the state of lifelong learning in service robots that has been published in Journal of Intelligent & Robotic Systems (<a href="https://link.springer.com/content/pdf/10.1007/s10846-021-01458-3.pdf" target="_blank" style="color: rgb(0,0, 150);">available online here</a>). 
													This survey paper covers all the topics of the cognitive robotics course in a concise and brief manner to help students in easy remembrance 
													and quick revision. I will try to share my knowledge with you throughout the course!
													<!-- , in which 75 students participated actively
													in both theoretical and practical sessions of the course. It is my pleasure that the students were satisfied with the content
													of the course and assessed it with a grade of 8.00! <b style="text-align: justify; color: rgb(0, 0, 255);"> This year, over 100 students have registered for this course, which can be considered 
													as a record for a master robotic course at the University of Groningen!</b> </p> -->
												</p>
												</p>
												<!-- <ul class="actions">
													<li><a href="http://www.ai.rug.nl/hkasaei" target="_blank" class="button">More</a></li>
												</ul> -->
											</div>
										</article>
									</div>
									
									

									</section>
									<section>
									<header class="major">
										<h2 id="OR">Teaching Assistants</h2>
									</header>

									This year, the course has six TAs to help you during the practical sessions: <b>Vlad, Subhaditya, Jeroen, Ruben, Abhishek, </b> and <b>Georgios</b>. 
									We would be happy to assist you to the best of our abilities, so don't hesitate to ask if you have any questions.
								</br></br>

									<div class="row gtr-50 gtr-uniform">
										
										
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
												<span class="image fit"><img src="images/vlad_photo.png" alt=""/>
												<p style="text-align: center; color: rgb(0, 0, 0);" ><b>Vlad Iftime</b>
														<br/> v.c.iftime@student.rug.nl </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										<!-- =============================================================================================== -->
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
													<span class="image fit"><img src="images/subhaditiya_photo.png" alt=""/>
													<p style="text-align: center; color: rgb(0, 0, 0);" > <b>Subhaditya Mukherjee </b>
														<br/> s.mukherjee.11@student.rug.nl </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										<!-- =============================================================================================== -->
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
												<span class="image fit"><img src="images/jeroen_photo.png" alt=""/>
												<p style="text-align: center; color: rgb(0, 0, 0);" ><b>  Jeroen Oude Vrielink </b>
														<br/>  j.oude.vrielink@student.rug.nl  </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										<!-- =============================================================================================== -->
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
												<span class="image fit"><img src="images/ruben_photo.png" alt=""/>
												<p style="text-align: center; color: rgb(0, 0, 0);" ><b> Ruben</b>
														<br/> r.b.h.spolmink@student.rug.nl  </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										<!-- =============================================================================================== -->
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
												<span class="image fit"><img src="images/Georgios.png" alt=""/>
												<p style="text-align: center; color: rgb(0, 0, 0);" ><b> Georgios Tziafas</b>
													<br/> g.t.tziafas@rug.nl </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										<!-- =============================================================================================== -->
										
										<div class="col-2">
											<!-- <a href="https://ieeexplore.ieee.org/document/9311749" target="_blank"> -->
											<a target="_blank">	
												<span class="image fit"><img src="images/abhishek_photo.png" alt=""/>
												<p style="text-align: center; color: rgb(0, 0, 0);" ><b> Abhishek Satyanarayana</b>
														<br/> a.ramanathapura.satyanarayana@student.rug.nl  </p>
												</span>
											</a>
										</div>
										<div class="col-0">
										</div>
										
										<!-- =============================================================================================== -->
										<div class="col-0">
										</div>
									<!-- <div class="posts">
										<article>
											<p style="text-align: justify;"> <span class="image left"><img src="images/thijs.png" alt=""></span> 
												<b style="color: rgb(0, 0, 0);">My name is Thijs Eker</b>. I'm a third year AI master student and am currently finishing my master thesis on ship classification. 
												I will be one of the TAs for this year's Cognitive Robotics course, if you have any questions don't hesitate to ask!
												<br><b style="color: rgb(0, 0, 200);"> Email: thijs.eker@gmail.com</b>										
											</p>												
										</article>
										<article>
											<p style="text-align: justify;"> <span class="image left"><img src="images/Georgios.png" alt=""></span> 
												<b style="color: rgb(0, 0, 0);">I am Georgios Tziafas</b>, a second year AI master student and an enthusiast of recent progress in the field of Machine Learning for Computer Vision and Natural Language Processing.
												My current work revolves mostly around Multi-Modal Learning for coupling visual perception to natural language understanding and inference, aiming at exciting AI applications for high-level interactive cognitive systems.
												Feel free to contact me!
												<br><b style="color: rgb(0, 0, 200);">Email: g.tziafas@student.rug.nl</b>
										
											</p>												
										</article>
										<article>
											<p style="text-align: justify;"> <span class="image left"><img src="images/Hamed.png" alt=""></span> 
												<b style="color: rgb(0, 0, 0);">I am Hamed Ayoobi</b>, a Ph.D researcher in ML, Computer Vision and Argumentation. 
												I am currently working on open-ended learning for 3D point cloud categorization and parts segmentation. 
												Moreover, I am working on a newly developed machine learning technique called Argumentaion-Based Learning (ABL). 
												I will be one of your TA's for Cognitive Robotics course and I would be happy to help you, if you have any questions.
												<br><b style="color: rgb(0, 0, 200);">Email: h.ayoobi@rug.nl</b>
											</p>												
										</article> -->

									</div>
								</section>								

							<!-- Section -->
								<section id="LO">
									<header class="major">
										<h2>Learning Objectives</h2>
									</header>
									<h3 style="text-align: justify; color: rgb(0, 0, 0);font-size: 22px;"> After successful completion of this course, students will be able to: </h3>
									<div class="features">
										<article>
											<span class="icon fa-diamond", style="color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Explain meaning of different concepts often used in the field of 3D object perception, grasping and 
													human robot interaction and their application in robotics.</p>												
											</div>
										</article>
										<article>
											<span class="icon fa-diamond", style="color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Explain the main theories of open-ended learning and cognitive robotics.</p>
											</div>
										</article>
										<article>
											<span class="icon fa-diamond", style=" color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Exploit deep transfer learning algorithms for open-ended object category learning and recognition.</p>
											</div>
										</article>
										<article>
											<span class="icon fa-diamond", style=" color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Implement and experiment several methods for object grasping.</p>
											</div>
										</article>
										<article>
											<span class="icon fa-diamond", style="color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Create a tight coupling between object perception and manipulation and perform experiment using real Kinect data
													and a simulated Panda robotic arm.</p>
											</div>
										</article>
										<article>
											<span class="icon fa-diamond", style="color: rgb(0, 0, 255);"></span>
											<div class="content">
												<p style="text-align: justify; color: rgb(0, 0, 0); ">Put hands-on experience working on a research project. It is expected that students will also gain the following 
													research skills: analyzing literature related to a particular topic, critiquing papers, and presentation of research ideas.</p>
											</div>
										</article>
									</div>
									

								</section>
								<!-- Section -->
								<section id="PQ">
										<header class="major">
											<h2>Prerequisites </h2>
										</header>
										<p style="text-align: justify; color: rgb(0, 0, 0); ">Prior knowledge of basic linear algebra is recommended, but not required. For programming throughout the course, 
											we mainly use C++/Python based ROS-melodic. For your final project, you are free to choose MATLAB, Python, or C++ as your coding language.
										</p>
								</section>
							<!-- Section -->
								<section id="CF">
									<header class="major">
										<h2>Class Format</h2>
									</header>

									
									<p style="text-align: justify; color: rgb(0, 0, 0); ">A cognitive robot should process very different types of information in varying time scales. Two different modes of 
										processing, generally labelled as System 1 and System 2, are commonly accepted theories in cognitive psychology. The operations of System 1 (i.e. perception and action) are typically fast,
										automatic, reactive and intuitive. The operations of System 2 (i.e. semantic) are slow, deliberative and analytic. 
										We recently wrote a survey paper about the 
										<b><a href = "https://rdcu.be/csdcK" target="_blank" class="external text" title="" rel="nofollow" style="color: rgb(0,0, 255);">State of Lifelong Learning in Service Robots</a></b>.
										It covers all the topics of the cognitive robotics course in a concise and brief manner to help students in easy remembrance and quick revision.

										</br></br>
										This year theme of the course is built on top of two important topics: <b><a href = "https://www.sciencedirect.com/science/article/pii/S0925231218302327" target="_blank" class="external text" title="" rel="nofollow" style="color: rgb(0,0, 255);">towards lifelong assistive robotics: a tight coupling between object perception and manipulation</a></b> 
										and <b><a href = "https://arxiv.org/pdf/2106.01866.pdf" target="_blank" class="external text" title="" rel="nofollow" style="color: rgb(0,0, 255);">simultaneous multi-view object grasping and recognition in open-ended domains</a></b>. 
										The course is a combination of lectures, reading sessions and robotic lab sessions. The lectures discuss the fundamentals of topics required to develop a cognitive robotic system mainly with
										the distinctive characteristics of System 1. During the reading sessions, students present and discuss recent contributions in the fields of object perception and manipulation. 
										See detailed contents, coursework and grading policies below.
									</p>
									<div class="posts">
										<article>
											<a href="#LT" class="image"><img src="images/pic01.jpg" alt="" /></a>
											<h3>Lectures</h3>
											<p style="text-align: justify;">Topics include Introduction to Cognitive Robotics, 3D Object Perception, Object Grasping and Manipulation, Planning, Human Robot Interaction, 
												Open-Ended Learning, Deep Transfer Learning, Evaluations, and Application to Assistive Robots.</p>
											<ul class="actions">
												<li><a href="#LT" class="button">Read More</a></li>
											</ul>
										</article>
										<article>
											<a href="#EA" class="image"><img src="images/pic02.jpg" alt="" /></a>
											<h3>Essay Assignment and Reading Sessions (15%)</h3>
											<p style="text-align: justify;"> An essay assignment (i.e., two pages <a href = "https://www.ieee.org/conferences/publishing/templates.html" target="_blank" style="color: rgb(0,0, 255);">IEEE conference format</a>)
												 has been designed to ensure all students will read important papers on 3D object perception, affordance
												 detection and manipulation.  All students will get a chance to present their essay.</p>
											<ul class="actions">
												<li><a href="#EA" class="button">Read More</a></li>
											</ul>
										</article>
										<article>
											<a href="#LW" class="image"><img src="images/grasping2.png" alt="" /></a>
											<h3>Robotic Lab Sessions (35%) + Final Project (50%)</h3>
											<p style="text-align: justify;">Two practical assignments have been devised to provide hands-on experiences for fundamental theories (35%). During the practical sessions, 
												we will use the RACE framework to get more insight into different algorithms. Then, students will work together on a finall project (50%). </p>
											<ul class="actions">
												<li><a href="#LW" class="button">Read more</a></li> <!-- LW:LabWork -->
											</ul>
										</article>
										
									</div>
								</section>
							<!-- Section -->
								<section id="LT">
									<header class="major">
										<h2>List of Topics</h2>
									</header>
									<p style="text-align: justify; color: rgb(0, 0, 0); ">The preliminary schedule is provided below and is subject to change. </p>

									<div class="table-wrapper">
											<table>
												<thead>
													<tr>
														<th>Session</th>
														<th>Description</th>
														<th>Date</th>
													</tr>
												</thead>
												<tbody>
																												
													<tr>
														<td>1</td>
														<td>Introduction to Cognitive Robotics</td>
														<td>20/9</td>
													</tr>
													<tr>
														<td>2</td>
														<td>3D Object Detection and Representation</td>
														<td>27/9</td>
													</tr>
													
													<tr>
														<td>3</td>
														<td> Open-Ended Object Category Learning and Recognition + Evaluation of object recognition</td>
														<td>4/10</td>
													</tr>
													<tr>
														<td>4</td>
														<td> Reading Day</td>
														<td>7/10</td>
													</tr>
													<tr>
														<td>5</td>
														<td>Object Grasping and Manipulation I </td>														
														<td>11/10</td>
													</tr>
													<tr>
														<td>6</td>
														<td> Reading Day</td>
														<td>14/10</td>
													</tr>
													<tr>
														<td>7</td>
														<td> Object Grasping and Manipulation 2</td>
														<td>18/10</td>
													</tr>
													<tr>
														<td>8</td>
														<td> Reading Day</td>
														<td>21/10</td>
													</tr>
													<tr>
														<td>9</td>
														<td> Reading Day</td>
														<td>28/10</td>
													</tr>
													<tr>
														<td>10</td>
														<td> Task/Path/Trajectory Planning and Memory Management  </td>
														<td>1/11</td>
													</tr>
													<tr>
														<td>11</td>
														<td> Reading Day</td>
														<td>4/11</td>
													</tr>
													<!-- <tr>
														<td>12</td>
														<td> TBD  </td>
														<td>1/11</td>
													</tr>	 -->
												</tbody>
											</table>
										</div>
									
								</section>
							<!-- Section -->
								<section id="CWQP">
										<header class="major">
											<h2>Coursework and Grading Policies</h2>
										</header>
										<p style="text-align: justify; color: rgb(0, 0, 0); ">Throughout the course, students will work partly individual and partly in groups of two on a related research project 
											that they present at the end of the course. The grading policy for this course is based on an essay assignment, practical works and a final project. Students will need to score at least
											 a 5.5 average, and will have to get at least a 5.0 on each component:
										</p>

										<!-- Essay assignment -->
										<h2 id="EA" style="color:  rgb(0, 0, 255);">1- Essay assignment (15%):</h2>
										
										<p style="text-align: justify;"><span class="image left"><img src="images/pic02.jpg" alt="" /></span> An essay assignment (i.e., two pages <a href = "https://www.overleaf.com/latex/templates/ieee-conference-template/grfzhhncsfqn" target="_blank" style="color: rgb(0,0, 255);">IEEE conference format</a>) has been 
											designed to ensure all students will read important papers on 3D object perception, affordance detection and manipulation. 
											All students will get a chance to present their essay throughout the class during the reading days. 
											Each group will presents the content of a (set of) relevant paper(s). 
											<b>The presenters need to send the essay of their reading assignments two dayes before the day of the class. </b> 
											As an example, if you are going to present your work on Wednesday, you need to submit your essay by midnight on Monday. </p>									
											<!-- Lists -->
											<h4>The grading policy for this assignment is based on a Rubric chart containing the following criteria (available on Nestor):</h4>
											<div class="row">
												<div class="col-20 col-30-small" >			
													<ul>
														<li style="color:  rgb(0, 0, 255);">Content/Delivery/Creativity/Organization/Ability to answer questions/Length of presentation/Overall	quality</li>
													</ul>
												</div>
											</div>
											<br>
											<p style="text-align: justify;"> Possible topics for the essay assignment include but not limited to the following papers. 
												It should be noted that you can use 
												<a href = "https://www.connectedpapers.com/" target="_blank" style="color: rgb(0,0, 255);">connected paper website</a> 
												to find related papers to the specific one. </p>

											<div class="table-wrapper">
													<table>
														<thead>
															<tr>
																<th>Topics</th>
																<th>Suggested Papers</th>																
															</tr>
														</thead>
														<tbody>
															
															<tr>
																<td>Object Perception</td>
																<td>
																	- Selecting CNN Features for Online Learning of 3D Objects <br> 
																	- Multi-View Convolutional Neural Networks<br>
																	- Unseen Object Instance Segmentation for Robotic Environments<br>
																	- Learning to See before Learning to Act: Visual Pre-training for Manipulation<br>
																	- Simultaneous Multi-View Object Grasping and Recognition in Open-Ended Domains<br>
																	- Improving Unimodal Object Recognition with Multimodal Contrastive Learning<br>
																	- Learning 3D Dynamic Scene Representations for Robot Manipulation
																	- Big Transfer (BiT): General Visual Representation Learning
																	- Few-shot Image Classification: Just Use a Library of Pre-trained Feature Extractors and a Simple Classifier
																	
																</td>																																
															</tr>
															<tr>
																<td>Continual Deep Learning</td>
																<td>
																	- A survey on Deep Learning Advances on Different 3D Data Representations<br>
																	- Replay in Deep Learning: Current Approaches and Missing Biological Elements<br>
																	- Continual Lifelong Learning with Neural Networks:A Review<br>
																	- A continual learning survey: Defying forgetting in classification tasks<br>
																	- Lifelong Learning with Dynamically Expandable Networks<br>
																	- 3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable Networks<br>
																	- OrthographicNet: A deep learning approach for 3D object recognition in open-ended domains<br>
																</td>																																
															</tr>																
															<tr>
																<td>Human Robot Interaction</td>
																<td>
																	- Reactive Human-to-Robot Handovers of Arbitrary Objects<br>
																	- Objects Affordance Recognition for Natural Language Human-Robot Interaction<br>
																	- Few-Shot Visual Grounding for Natural Human-Robot Interaction<br>
																	- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields<br>
																	- SHPR-Net: Deep Semantic Hand Pose Regression From Point Clouds<br>
																	- Object affordance based multimodal fusion for natural Human-Robotinteraction<br>
																	- Semantic Scene Manipulation Based on 3D Spatial Object Relations and Language Instructions<br>															
																	- Learning One-Shot Imitation from Humans without Humans
																	- Human-to-Robot Imitation in the Wild


																</td>
															</tr>			
															<!-- <tr>
																<td>3D Object Recognition</td>
																<td>OrthographicNet: A Deep Learning Approach for 3D Object Recognition in Open-Ended Domains</td>																																
															</tr> -->
															<tr>
																<td>Object Affordance Detection</td>
																<td>
																	- Learning to Grasp 3D Objects using Deep Residual U-Nets<br>
																	- Affordance detection of tool parts from geometric features<br>
																	- Robotics Dexterous Grasping: The Methods Based on Point Cloud and Deep Learning (review)<br>
																	- AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection<br>
																	- Object affordance detection with relationship-aware network<br>
																	- Affordance Detection of Tool Parts from Geometric Features<br>
																	- One-Shot Object Affordance Detection in the Wild<br>
																	- Robotic pick-and-place of novel objects in clutter with multiaffordance grasping and cross-domain image matching

																</td>										
															</tr>
															<tr>
																<td>Shape Completion for Grasping</td>
																<td> 
																	- Shape Completion Enabled Robotic Grasping*<br>
																	- Beyond Top-Grasps Through Scene Completion<br>
																	- Multi-Modal Geometric Learning for Grasping and Manipulation<br>
																	- RGB-D Local Implicit Function for Depth Completion of Transparent Objects<br>
																	- 3D Shape Estimation of Transparent Objects for Manipulation<br>
																	- Task-driven Perception and Manipulation for Constrained Placement with No Shape Prior<br>
																	- Learning 3D Shape Completion Under Weak Supervision<br>
																	- Diverse Plausible Shape Completions from Ambiguous Depth Images

																</td>
															</tr>

															<tr>
																<td>Object Grasping and Maniulation</td>
																<td> 
																	- MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered Environments<br>
																	- Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach<br>
																	- Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter<br>
																	- High precision grasp pose detection in dense clutter<br>
																	- 6-DOF Grasping for Target-driven Object Manipulation in Clutter<br>
																	- Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics<br>
																	- PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds<br>
																	- Challenges and Outlook in Robotic Manipulation of Deformable Objects	<br>
																	- Self-Imitation Learning by Planning <br>
																	- Fast geometry-based computation of grasping points on three-dimensional point clouds															
																	- SafePicking: Learning Safe Object Extraction via Object-Level Mapping
																	- 

																</td>
															</tr>
															
															
															<tr>
																<td>Kinestectich Teaching / Learning from Demonstrations</td>
																<td>
																	- Learning to Grasp Familiar Objects using Object View Recognition and Template Matching<br>
																	- Learning of grasp selection based on shape-templates<br>
																	- Learning robots to grasp by demonstration<br>
																	- Virtual Kinesthetic Teaching for Bimanual Telemanipulation<br>
																	- Learning Dexterous Grasping with Object-Centric Visual Affordances<br>
																	- Robot Grasping in Clutter:Using a Hierarchy of Supervisors for Learning from Demonstrations<br>
																	- Grasping in the Wild: Learning 6DoF Closed-Loop Grasping From Low-Cost Demonstrations<br>
																	- Learning Interactively to Resolve Ambiguity in Reference Frame Selection<br>
																	- Recent Advances in Robot Learning from Demonstration<br>
																	- A Learning-from-Observation Framework: One-Shot Robot Teaching for Grasp-Manipulation-Release Household Operations


																</td>
															</tr>

															<tr>
																<td>Task-Informed/Oriented Grasping</td>
																<td>
																	- Learning task-oriented grasping for tool manipulation from simulated self-supervision<br>
																	- KETO: Learning Keypoint Representations for Tool Manipulation Tool Manipulation<br>
																	- OmniHang: Learning to Hang Arbitrary Objects <br>
																	- Learning Task-Oriented Grasping From Human Activity Datasets <br>
																	- Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping<br>
																	- Category-based task specific grasping<br>
																	- Part-based Grasp Planning for Familiar Objects <br>
																	- Same Object, Different Grasps: Data and Semantic Knowledge for Task-Oriented Grasping

																</td>
															</tr>
															
																																						
															<tr>
																<td> Dual-Arm Collaborative Manipulation</td>
																<td> 
																	- Autonomous Dual-Arm Manipulation of Familiar Objects<br>
																	- Autonomous Bimanual Functional Regrasping	of Novel Object Class Instances<br>
																	- Grasping Bulky Objects with Two Anthropomorphic Hands<br>
																	- Learning a Decentralized Multi-arm Motion Planner<br>
																	- Learning and Generalisation of Primitives Skills Towards Robust Dual-arm Manipulation<br>
																	- Deep Visual Reasoning: Learning to Predict Action Sequences for Task and Motion Planning from Images<br>
																	- Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input<br>
																	- A Novel Pipeline for Bi-manual Handover Task<br>
																	- Vision-Based Robotic Pushing and Grasping for Stone Sample Collection under Computing Resource Constraints<br>
																	- Dual arm manipulation - A survey<br>
																	- Interactive, Collaborative Robots: Challenges and Opportunities<br>
																	- Fully Autonomous Picking with a Dual-ArmPlatform for Intralogistics

																</td>
															</tr>
															<tr>
																<td> Pile Segmentation and Manipulation</td>
																<td> 
																	- Perceiving, Learning, and Exploiting Object Affordances for Autonomous Pile Manipulation	<br>
																	- Interactive singulation of objects from a pile <br>
																	- Learning to Singulate Objects using a Push Proposal Network <br>
																	- Model-Based Grasping of Unknown Objects from a Random Pile<br>
																	- Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning<br>
																	- Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching<br>
																	- A Learning-based Robotic Bin-picking with Flexibly Customizable Grasping Conditions<br>
																	- Probabilistic segmentation and targeted exploration of objects in cluttered environments

																</td>
															</tr>
															<tr>
																<td> Sim2Real Transfer</td>
																<td> 
																	- Closing the Simulation-to-Reality Gap for Deep Robotic Learning<br>
																	- Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks<br>
																	- Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?<br>
																	- Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control<br>
																	- Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping<br>
																	- Transferring Grasp Configurations using Active Learning and Local Replanning
																	<!-- - Graph-based Hierarchical Knowledge Representation for Robot Task Transfer from Virtual to Physical World -->

																</td>
															</tr>

															<tr>
																<td> 6D pose estimation</td>
																<td> 
																	- Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd<br>
																	- Robust 6D Object Pose Estimation in Cluttered Scenes using Semantic Segmentation and Pose Regression Networks<br>
																	- Recovering 6D Object Pose: A Review and Multi-modal Analysis<br>
																	- DONet: Learning Category-Level 6D Object Pose and Size Estimation	from Depth Observation<br>
																	- Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge<br>
																	- Multi-view 6D object pose estimation and camera motion planning using RGBD images<br>
																	- Multi-view  Self-supervised Deep Learning for 6D Pose Estimationin the Amazon Picking Challenge
																</td>
															</tr>
															<tr>
																<td> Visual Servoing</td>
																<td> 
																	- Survey on Visual Servoing for Manipulation<br>
																	- Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks<br>
																	- End-to-End Training of Deep Visuomotor Policies<br>
																	- Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection<br>
																	- Dynamic Grasping with Reachability and Motion Awareness
																	
																</td>
															</tr>
														
															<tr>
																<td> Assistive Robotics</td>
																<td> 
																	- Robots for Humanity Using Assistive Robotics to Empower People with Disabilities<br>
																	- Adaptive Robot-Assisted Feeding: An Online Learning Framework for Acquiring Previously-Unseen Food Items<br>
																	- Is More Autonomy Always Better? Exploring Preferences of Users with Mobility Impairments in Robot-assisted Feeding<br>
																	- Robot-Assisted Feeding: Generalizing Skewering Strategies across Food Items on a Plate<br>
																	- Transfer depends on Acquisition: Analyzing Manipulation Strategies for Robotic Feeding<br>
																	- Towards Robotic Feeding: Role of Haptics in Fork-based Food Manipulation<br>
																	- Sensing Shear Forces During Food Manipulation: Resolving the Trade-Off Between Range and Sensitivity
																</td>
															</tr>
															<!-- <tr>
																<td> Lessons from Babies</td>
																<td> The Development of Embodied Cognition: Six Lessons from Babies	</td>
															</tr> -->
															
																
															<tr>
																<td> ...</td>
																<td> ... </td>
															</tr>	

														</tbody>
													</table>
												</div>

										</p>
										<!-- 2- Practical assignment -->

										<h2 id="LW" style="color:  rgb(0, 0, 255); ">2- Practical assignments and report (35%):</h2>
										
										<p style="text-align: justify;">
											<!-- <span class="image right"><img src="images/pic03.jpg" alt="" id="LW" /></span><span class="image right"><img src="images/pic032.jpg" alt="" id="LW" /></span>  -->
											Two practical assignments have been devised to provide hands-on experiences for fundamental theories. 
											During the practical sessions, we will use the RACE framework, Gazebo, Rviz, MoveIt, and standard datasets to get more insight into different algorithms. Particularly, 
											students will work on a “clear_table” scenario using the RACE framework, which composes of two projects: </p>														
											<div class="row">
												<div class="col-10 col-30-small">
													<ul>														
													<li> <b style="color: rgb(0, 0, 255);">Project1:</b> <b style="color: rgb(0, 0, 0);"> Open-Ended Learning Approaches for 3D Object Recognition. </b></li>
													<li> <b style="color: rgb(0, 0, 255);">Project2:</b> <b style="color: rgb(0, 0, 0);"> Coupling between Object Perception and Manipulation: Learning to Grasp Objects in Highly Cluttered Environments</b></li>																	
													</ul>
												</div>
											</div>				
											<p style="text-align: justify;">For both projects, students are provided with stub of the code and are expected to complete it (see the details below).
												During the first and second assignments, students have to think about their final project, in particular, what they want to develop and if possible which comparisons they want to make. 
												
											</br></br>
											
												<b style="color: rgb(255, 0, 25);">*Note:</b> <b> At the end of each practical assignment a report (i.e., up to four pages <a href = "https://www.overleaf.com/latex/templates/ieee-conference-template-example/nsncsyjfmpxy" target="_blank" style="color: rgb(0,0, 255);">IEEE conference format</a>),
													including all the figures, tables, and references, has to be delivered.</b> These practical assignments prepare students to do the final course project. Please read the detailed description of each project below.</br>
												
												<p style = "text-align: justify;"><b style="color: rgb(0, 0, 255);">*Extra credit:</b> We will evaluate your object recognition system using an open-ended evaluation protocol. 
													We will add <b style="color: rgb(0, 0, 255);">0.5 point</b> to the final score of the student who achieves the highest performance, <b style="color: rgb(0, 0, 255);">0.35 point</b>  
													to the student who achieves the second place, and <b style="color: rgb(0, 0, 255);">0.20 point</b>  to the student who achieves third place. We will compute the performance 
													of your algorithm ourselves (code that does not run will be disqualified from the contest). This reward is designed to encourage you to experiment with different algorithms 
													and hyperparameter settings to obtain the best performance.</p>		
											<!-- Lists -->
											<h4>The grading policy for this assignment is based on a Rubric chart containing the following criteria (will be available on Nestor):</h4>
											<div class="row">
												<div class="col-20 col-30-small">

													<ul>
															
														<li style="color:  rgb(0, 0, 255);">Content/Delivery/Creativity and presentation/Organization/Neatness and attractiveness/Overall quality</li>
													
													</ul>
												</div>
											</div>				
											
										</br>
											<div class="posts">
													<article>
														<a class="image"><img src="images/pic033.jpg" alt="" /></a>
														<h4 style="text-align: justify;"> Setup the RACE framework and its dependencies	</h4>
														<p style="text-align: justify;">
																Here you will discover how easy it is to get started with the RACE framework.
																This tutorial is not meant to be a deep dive into the ROS functionalities or codes surrounding object perception and learning.
																Here we just refer the students to the installation steps required for each project. 
																If you’re interested in studying the framework in depth, including both (1) discussion of theories and (2) hands-on implementations, 
																check out <a href = "https://rugcognitiverobotics.github.io/documents/Hamidreza_Kasaei_PhD_thesis.pdf" target="_blank" style="color: rgb(0,0, 255);">my Ph.D. thesis</a>.
																
 
														</p>
														<ul class="actions">
															<!-- <li><a href="https://rugcognitiverobotics.github.io/setup_2020.html" target="_blank" class="button">Read More</a></li> -->
															<!-- <li><a href="" target="_blank" class="button">Read More</a></li> -->
															<!-- <a class="button small" href="" target="_blank">Read More (html)</a>  -->
															<a class="button small" href="https://github.com/SeyedHamidreza/cognitive_robotics_ws" target="_blank"> Project 1 </a> 
															<a class="button small" target="_blank"> Project 2 </a> 


														</ul>
													</article>
													<article>
														<a href="https://youtu.be/iEq9TAaY9u8" target="_blank" class="image"><img src="images/pic032.jpg" alt="" /></a>
														<h4 style="text-align: justify;">[Project1] Open-Ended Learning Approaches for 3D Object Recognition</h4>
														<p style="text-align: justify;"> Human beings learn to recognize object categories ceaselessly over time. This ability to refine and extend knowledge from the set of 
															accumulated experiences facilitates the adaptation to new environments. In this assignment, studnets will learn how to create a lifelong object recognition system 
															that can learn about 3D object categories in an open-ended fashion [<a href="https://youtu.be/iEq9TAaY9u8" target="_blank">see the example</a>].
														</p>
														<ul class="actions">
																<!-- <li><a href="https://youtu.be/iEq9TAaY9u8" target="_blank" class="button">Read More about Project 1</a></li> -->
																<!-- <li><a href="https://www.dropbox.com/s/6b0ibgcqor5j5eq/project1.pdf?dl=1" target="_blank"  class="button">Read More about Project 1 (PDF)</a></li> -->
																<!-- <li><a href="https://www.dropbox.com/s/pba0wwidf9yst2x/description_of_project2_2021.pdf?dl=1" target="_blank"  class="button">Read More about Project 1 (PDF)</a></li> -->

															<a class="button small" href="https://github.com/SeyedHamidreza/cognitive_robotics_ws" target="_blank"> GitHub </a> 
															<a class="button small" href="https://rugcognitiverobotics.github.io/documents/description_of_project1_2021_part1.pdf" target="_blank" > Part 1 (PDF) </a> 
															<a class="button small" href="https://rugcognitiverobotics.github.io/documents/description_of_project1_2021_part2.pdf" target="_blank"> Part 2 (PDF) </a> 
												
														</ul>
													</article>
													<article>
														<a target="_blank" class="image"><img src="images/grasping2.png" alt="" /></a>
														<!-- <a target="_blank" class="image"><img src="videos/pile.gif" height="256"/></a> -->
														<h4 style="text-align: justify;">[Project2] Coupling between Object Perception and Manipulation </h4>
														<!-- <p style="text-align: justify;">In this project, students will work on a virtual reality scenario. In particular, a virtual Panda robotic arm works on the real-world environment.
															 The robot perceives the environment using a real Kinect camera; it has to detect, learn and recognize objects and then, grasp 
															 the object. Students will learn how to couple perception and manipulation [<a href="https://youtu.be/6QNwps1TjWA" target="_blank">see the example</a>].
															</p> -->
															<p style="text-align: justify;">In this assignment, students will learn about the concept of visual grasping and manipulation.  
																In particular, we developed a simulation environment in the PyBullet, where a UR5e robot perceives the environment using an RGB-D camera (e.g., Kinect);
																In this assignment, students should program the robot to be able to grasp and manipulate objects in different situations.										
															   </p>
														<ul class="actions">
																
																<!-- <li><a href="https://youtu.be/6QNwps1TjWA" target="_blank" class="button">Read More about Project 2</a></li> -->
																<!-- <li><a href="https://www.dropbox.com/s/9mdgk028as5l8sm/project2.pdf?dl=1" target="_blank" class="button">Read More about Project 2 (PDF)</a></li> -->

																<a class="button small" href="https://rugcognitiverobotics.github.io/documents/description_of_project2_2021.pdf" target="_blank">Instruction (PDF)</a> 
																<a class="button small" href="https://github.com/SeyedHamidreza/cognitive_robotics_manipulation" target="_blank">GitHub</a> 

														</ul>
													</article>
													
												</div>
											
										</p>

								<table>
									<thead>
										<tr>
											<th></th>
											<th></th>																
										</tr>
									</thead>
								</table>
										<!-- 3- Final project -->
									</br>
										<h2 style="color:  rgb(0, 0, 255); ">3- Final project and report (50%):</h2>
										
										<p style="text-align: justify;"><span class="image left"><img src="images/pic04.jpg" alt="" id="LW" /></span> For the final project, students will work in groups of four on either a 
											specific research project (up to 25% materials can be borrowed from the previous assignments) or an in-depth literature survey (~20-30 relevant papers, organized by different features, 
											identifying gaps in the state of the art). Finally, each group will write a report with the structure of a scientific conference paper 
											(i.e., <b> 6+n pages</b> <a href = "https://www.overleaf.com/latex/templates/ieee-conference-template-example/nsncsyjfmpxy" target="_blank" style="color: rgb(0,0, 255);">IEEE 
											conference format</a>). The report material (including text, figures, tables, acknowledgment, etc.) must fit into 6 pages, while there is no page limit for the appendix/references (n pages) sections.  
											<b> Reports exceeding the (6+n) page limit will not be evaluated. </b> The last two sessions are dedicated to the final project presentations.
											Each group has to send a short description of the project (up to 2 paragraphs) to Hamidreza Kasaei (cognitiverobotic@gmail.com) to get feedback about the proposed project. This document should state:
										</p>														
											<div class="row">
												<div class="col-10 col-30-small">
													<ul>
														<li> Title of project, student names and student numbers,</li>
														<li> Explain the goal of the to-be-developed system,</li>
														<li> Explain how will you compare your approach with other state-of-the-art approaches if possible.</li>							
													</ul>
												</div>
											</div>									
											<p style="text-align: justify;"> Detailed information about the content of the report and the deadline will be posted later. Here is a short list of possible projects. 
												You may want to check out my list of projects and publications (see <a href = "https://www.ai.rug.nl/irl-lab/publications.html" target = "_blank" style="color: rgb(0, 0, 255);">https://www.ai.rug.nl/irl-lab/publications.html</a>).
												You should select a topic from the below list and develop/test your idea using the provided PyBullet framework, 
												or come up with another project idea on your own (the topic should be relevant to the content of the course).
												If you prefer to work on a c++ based framework, please contact Hamidreza Kasaei (course coordinator). 
												
												<div class="table-wrapper">
														<table>
															<tbody>
																								
																<tr>
																	<td>Active Object Recognition</td>
																	<td>Shape Completion for Grasping</td>	
																	<td>Pile Segmentation and Manipulation</td>																															
																</tr>
																<tr>
																	<td>Affordance Detection</td>
																	<td>6D pose estimation</td>
																	<!-- Real-Time 3D Segmentation of Cluttered Scenes for Robot Grasping -->
																	<td>Active Segmentation of Cluttered Scenes</td>										
																</tr>
																<tr>
																	<td>Grasping and Manipulation</td>
																	<td>Dual-Arm Manipulation </td>
																	<td>Task-Informed/Oriented Grasping	</td>										
																</tr>
																<tr>
																	<td>Human-Robot Interaction</td>
																	<td>Deep Transfer Learning</td>
																	<td>Eye-in-Hand and Eye-to-Hand System</td>										
																</tr>
																<tr>
																	<td>Open-Ended learning</td>										
																	<td>RL for Manipulation</td>
																	<td>Path Planning in Manipulation</td>										
																</tr>
															</tbody>
														</table>
													</div>
	
											<!-- Lists -->
											<h4>The grading policy for this assignment is based on a Rubric chart containing the following criteria (will be available on Nestor):</h4>
											<div class="row">
												<div class="col-20 col-30-small">

													<ul>
															
														<li style="color:  rgb(0, 0, 255);">Content/Delivery/Creativity and presentation/Organization/Neatness and attractiveness/Overall quality</li>
													
													</ul>
												</div>
											</div>				
										</section>
										<section id="Remarks">
													<header class="major">
														<h2>Rmarks</h2>
													</header>
													<div class="row">
														<div class="col-20 col-30-small">

															<ul>
																<li> <b>Late submissions</b> will lose <b>25%</b>, <b>50%</b>, and <b>100%</b> of the initial mark per day respectively. Therefore, having <b>0</b> marks on the third day after the deadline. 
																	You may submit your first practical assignment late (up to two days) with no penalty; this will be applied <b> ONLY to your first practical assignment</b>.
																<li>All the page limits <b>exclude</b> references.</li>
																<li>Feel free to collaborate on solving the problem but write your code individually. In particular, do not copy code from other students.</li>
															</ul>
														</div>
													</div>
													<!-- This class is partially based on the following existing courses:  -->
											<!-- <div class="row"> -->
													<!-- <div class="col-10 col-30-small"> -->
														<!-- <ul> -->
															<!-- <li> TBD.</li>																			 -->
														<!-- </ul> -->
													<!-- </div> -->
												<!-- </div>			 -->
										</p>
									</section>

							
							
						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									
									<ul>
										
										<li><a href="#banner">Course Description</a></li>
										<li><a href="#CS">Course Staff</a></li>
										<li><a href="#LO">Learning Objectives</a></li>
										<!-- <li><a href="#PQ">Prerequisites</a></li> -->
										<li><a href="#CF">Class Format</a></li>
										<li><a href="#LT">List of Topic</a></li>
										<li><a href="#CWQP">Coursework and Grading Policies</a></li>
										<li><a href="#Remarks">Remarks</a></li>

									</ul>
								</nav>


							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<p> <b style="color: rgb(255, 0, 0);">Hamidreza Kasaei</b> (Course Coordinator)</p>
									<ul class="contact">
										<li class="fa-envelope-o">hamidreza.kasaei@rug.nl</li>
										<li class="fa-dribbble"><a href="http://www.ai.rug.nl/hkasaei" target="_blank"> www.ai.rug.nl/hkasaei</a></li>
										<li class="fa-home">Office #340, Bernoulliborg building, University of Groningen.</li>
									</ul>
									<p> <b style="color: rgb(255, 0, 0);">Teaching Assistants</b>
									<ul class="contact">
										<!-- <li class="fa-envelope-o">cognitiverobotic@gmail.com</li> -->
										<li class="fa-envelope-o">a.l.miculita@student.rug.nl</li>
										<li class="fa-envelope-o">v.c.iftime@student.rug.nl</li>
										<li class="fa-envelope-o">a.toca@student.rug.nl</li>				
										<li class="fa-envelope-o">k.voncina@student.rug.nl</li>				

									<!--<li class="fa-home"><a href="#">Office #350, Bernoulliborg building, University of Groningen. </a></li>-->
									</ul>
				
								</section>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
